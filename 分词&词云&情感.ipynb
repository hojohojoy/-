{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'China','Deutschland','Europa','USA','TEUR','Unternehmen','Welt','İşbank','Geschäftsjahr','EU','Euro','neue','Höhe','deutschen','neuen','Entwicklung','Merkel','Trump','Land','Peking','EUR','deutsche','Wirtschaft','Schicksalsgemeinschaft','Xi','chinesische','Ende','Berlin','Bank','Menschen','Türkei','Länder','chinesischen','Zeit','Präsident','Risiken','Verbindlichkeiten','Zukunft','Bereich','Wohnungen','Investitionen','Regierung','Staaten','Russland','Politik','Wachstum','Zusammenarbeit','zusammen','Vergleich','Li','China','Deutschland','Europa','USA','Welt','Virus','Russland','EU','Menschen','Land','Pandemie','chinesischen','deutschen','deutsche','Berlin','Politik','Wirtschaft','neue','Länder','Frage','Schicksalsgemeinschaft','Unternehmen','Ukraine','Italien','BSZ','neuen','Deutschlands','Taiwan','Ende','Corona','Demokratie','Weber','Zeit','Peking','Deutschen','globalen','Xi','Menschheit','Krise','Europas','Ländern','Zukunft','Staaten','politischen','chinesische','Entwicklung','Westen','europäischen','beiden','Krieg',"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 7500x7500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wordcloud\n",
    "from nltk import word_tokenize, sent_tokenize, FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from matplotlib import rcParams, pyplot as plt\n",
    "import csv #调用数据保存文件\n",
    "import pandas as pd #用于数据输出\n",
    "\n",
    "#设置字体\n",
    "rcParams[\"font.size\"] = 8\n",
    "rcParams['figure.figsize'] = (10.0, 10.0) \n",
    "plt.style.use('ggplot')\n",
    "plt.figure(figsize=(15,15),dpi=500) #设置图片大小和像素\n",
    "\n",
    "#下载所需数据集\n",
    "#nltk.download()\n",
    "\n",
    "#架在停用词\n",
    "stop_words = list(stopwords.words('german')) # 停用词表，去掉不需要的词\n",
    "for i in range(len(stop_words)):\n",
    "    stop_words.append(stop_words[i].capitalize())\n",
    "stop_words.extend(('immer','Chinas','müssen','wurde','schon','heute','viele','Jahren','gerade','Prozent','wäre','dafür','eigenen','weniger','wurden','Milliarden','sehen','kommen','großen','gegenüber','lange','große','brauchen','steht','inzwischen','lässt','Jahre','unserer','vielen','Blick','macht','wirklich','Beispiel','bereits','darauf','wohl','besonders','ersten','fast','geben','tun','hätte','wenig','denen','vergangenen','sollten','damals','darüber','eigene','sogar','klar','lassen','seien','heißt','stellen','kommt','sollen','natürlich','erste','Wochen','Fall','davon','einfach','gehen','stehen','Wer','innen','Woche','gemacht','bleiben','pro','derzeit','täglich','halten','darf','später','deutlich','dabei','braucht','scheint','zurück','daher','sagte','allerdings','helfen','sieht','nie','U','Vorjahr','Jahren','wurde','immer','Prozent','wurden','müssen','Jahre','viele','sagte','drei','gegenüber','heute','Millionen','großen','fünf','schon','Milliarden','bereits','seien','steht','Dienstag','unserer','dafür','Uhr','gerade','weiterhin','insbesondere','wäre','sieht','Mio','kommt','insgesamt','Ve','sogar','große','deutlich ','sagen','kommen','dabei','vielen','vier','aufgrund','bzw','sollen','lange','worden','davon','konnte','Donald','lassen','bleiben','bleibt','derzeit','geworden','ersten','allerdings','sehen','größten','fast','Tag','sollten','wenig','darauf','Angela','heißt','zehn','gilt','letzten','wegen','stehen','jedoch','Dezember','denen','İş','Bankası','Beispiel','liegt','Seite','Woche','macht','eigenen','tun','stellen','nie','a','sowie','jahr','Jahr','t€','T€','gfr','mehr','prozent','uhr','sei','ab','AG','T','gut','mal','seit','sagt','gibt','ganz','wer','ende','teur','rund','jahr','ja','dach','DACH','beim','jahr','geht','gar','zwei','erst','etwa'))\n",
    "\n",
    "all_words=[]\n",
    "emo_dict={}\n",
    "\n",
    "#文本清理\n",
    "def cleanText(text):\n",
    "    #分句\n",
    "    sents=sent_tokenize(text)\n",
    "    for sent in sents:\n",
    "        #分词、去除停用词\n",
    "        word_tokens=word_tokenize(sent)\n",
    "        for w in word_tokens:\n",
    "            if (w not in stop_words) and w[0].isalpha():\n",
    "                all_words.append(w)\n",
    "    #print(all_words[0])\n",
    "\n",
    "#统计词频并绘制图表\n",
    "def printFreq():\n",
    "    freq_dist = FreqDist(all_words)\n",
    "    i=0\n",
    "    '''\n",
    "    labels=[]\n",
    "    x=[]\n",
    "    y=[]\n",
    "    for k in freq_dist:\n",
    "        i+=1\n",
    "        if i>50:\n",
    "            break\n",
    "        labels.append(k+' ('+str(i)+')')\n",
    "        x.append(i)\n",
    "        y.append(freq_dist[k])\n",
    "    plt.plot(x,y)\n",
    "    plt.xticks(range(50),labels=labels,rotation=90) #x轴坐标轴\n",
    "    '''\n",
    "    for k in freq_dist:\n",
    "        i+=1\n",
    "        if i>50:\n",
    "            break\n",
    "        print(\"'\"+k+\"',\",end='')\n",
    "\n",
    "#绘制词云图\n",
    "def drawCloud(beforeText,afterText):\n",
    "    wc = wordcloud.WordCloud(font_path=\"msyh.ttc\",\n",
    "                            width = 1000,\n",
    "                            height = 700,\n",
    "                            background_color='white',\n",
    "                            max_words=50,\n",
    "                            stopwords=stop_words)\n",
    "    wc.generate(beforeText) # 加载词云文本\n",
    "    wc.to_file(\"cloud_before.png\") # 保存词云文件\n",
    "    wc.generate(afterText)\n",
    "    wc.to_file(\"cloud_after.png\")\n",
    "\n",
    "#构造情感词典\n",
    "def init_dict():\n",
    "    emo=0\n",
    "    count=0\n",
    "\n",
    "    #打开两个字典文件\n",
    "    for file in ['SentiWS_v2.0_Negative.txt','SentiWS_v2.0_Positive.txt']:\n",
    "        f=open(file)\n",
    "        lines=f.readlines()\n",
    "        for line in lines:\n",
    "            words=line.split('\t')\n",
    "            word_cha=words[0]\n",
    "            weight=words[1]\n",
    "            other_forms=words[2]\n",
    "\n",
    "            if '-' in weight:\n",
    "                w = -1\n",
    "            else:\n",
    "                w = 1\n",
    "            emo+=float(weight)\n",
    "            count+=1 #计算总次数\n",
    "\n",
    "            index=word_cha.find('|')\n",
    "            if 0 and '选择计算情感的方法--------------------------------------------------':\n",
    "                emo_dict[word_cha[:index]]=w\n",
    "                words=other_forms.split(',')\n",
    "                for word in words:\n",
    "                    word = word.strip()\n",
    "                    emo_dict[word]=w\n",
    "            else:\n",
    "                emo_dict[word_cha[:index]]=weight\n",
    "                words=other_forms.split(',')\n",
    "                for word in words:\n",
    "                    word = word.strip()\n",
    "                    emo_dict[word]=weight\n",
    "        f.close()\n",
    "    #print(emo_dict)\n",
    "    print('词典中平均每词带有情感：'+str(emo/count)[:9]+'\\n')\n",
    "#计算文本情感\n",
    "def count_emo(s):\n",
    "    posi_emo=0\n",
    "    nega_emo=0\n",
    "    posi_count=0\n",
    "    nega_count=0\n",
    "\n",
    "    emo=0\n",
    "    count=0\n",
    "    '''\n",
    "    for word in all_words:\n",
    "        #当字典中有该词时，改变emo值\n",
    "        if word in emo_dict:\n",
    "            count=count+1\n",
    "            emo+=float(emo_dict[word])\n",
    "    '''\n",
    "    for word in all_words:\n",
    "        #当字典中有该词时，改变emo值\n",
    "        if word in emo_dict:\n",
    "            amount=float(emo_dict[word])\n",
    "            if amount>0:\n",
    "                posi_count+=1\n",
    "                posi_emo+=amount\n",
    "            else:\n",
    "                nega_count+=1\n",
    "                nega_emo+=amount\n",
    "            \n",
    "    print(s+'，积极情感词数：'+str(posi_count))\n",
    "    print(s+'，积极情感总值：'+str(posi_emo)[:9])\n",
    "    print(s+'，积极词平均情感：'+str(posi_emo/posi_count)[:9]+'\\n')\n",
    "    print(s+'，消极情感词数：'+str(nega_count))\n",
    "    print(s+'，消极情感总值：'+str(nega_emo)[:9])\n",
    "    print(s+'，消极词平均情感：'+str(nega_emo/nega_count)[:9]+'\\n')\n",
    "\n",
    "def main():\n",
    "    # 分别从语料库读取疫情前后的文本\n",
    "    with open(\"myCorpus_before.txt\",encoding=\"utf-8\") as f:\n",
    "        beforeText = f.read()\n",
    "    with open(\"myCorpus_after.txt\",encoding=\"utf-8\") as f:\n",
    "        afterText = f.read()\n",
    "\n",
    "    #init_dict()\n",
    "\n",
    "    cleanText(beforeText)\n",
    "    printFreq()\n",
    "    #count_emo('疫情前')\n",
    "\n",
    "    all_words.clear()\n",
    "\n",
    "    cleanText(afterText)\n",
    "    printFreq()\n",
    "    #count_emo('疫情后')\n",
    "\n",
    "    #drawCloud(beforeText,afterText)\n",
    "\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "49cb93f377a7abe7414b7b0f21fb3017538004a126cf690fb524202736b7fb92"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
