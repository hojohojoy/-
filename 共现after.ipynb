{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94\n",
      "写入文件成功,请在node_after.csv中查看\n",
      "写入文件成功,请在edge_after.csv中查看\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "import json\n",
    "\n",
    "co_ist=[]\n",
    "#为减少共现关系中的词语数，只在如下列表中选取字符\n",
    "useful_words=['China','Deutschland','Europa','USA','Welt','Virus','Russland','EU','Menschen','Land','Pandemie','deutschen','deutsche','chinesischen','Berlin','Politik','Wirtschaft','neue','Frage','Unternehmen','Ukraine','Italien','Länder','BSZ','Schicksalsgemeinschaft','neuen','Deutschlands','Taiwan','Ende','Demokratie','Weber','Corona','Peking','Deutschen','globalen','Xi','Zeit','Krise','Europas','Menschheit','Ländern','Zukunft','Staaten','politischen','Entwicklung','Westen','europäischen','Krieg','chinesische','beiden']\n",
    "\n",
    "def my_cut(text): \n",
    "    # 加载停用词\n",
    "    stop_words = list(stopwords.words('german')) # 停用词表，去掉不需要的词\n",
    "    for i in range(len(stop_words)):\n",
    "        stop_words.append(stop_words[i].capitalize())\n",
    "    stop_words.extend(('immer','w','http','Chinas','müssen','wurde','schon','heute','viele','Jahren','gerade','Prozent','wäre','dafür','eigenen','weniger','wurden','Milliarden','sehen','kommen','großen','gegenüber','lange','große','brauchen','steht','inzwischen','lässt','Jahre','unserer','vielen','Blick','macht','wirklich','Beispiel','bereits','darauf','wohl','besonders','ersten','fast','geben','tun','hätte','wenig','denen','vergangenen','sollten','damals','darüber','eigene','sogar','klar','lassen','seien','heißt','stellen','kommt','sollen','natürlich','erste','Wochen','Fall','davon','einfach','gehen','stehen','Wer','innen','Woche','gemacht','bleiben','pro','derzeit','täglich','halten','darf','später','deutlich','dabei','braucht','scheint','zurück','daher','sagte','allerdings','helfen','sieht','nie','U','Vorjahr','Jahren','wurde','immer','Prozent','wurden','müssen','Jahre','viele','sagte','drei','gegenüber','heute','Millionen','großen','fünf','schon','Milliarden','bereits','seien','steht','Dienstag','unserer','dafür','Uhr','gerade','weiterhin','insbesondere','wäre','sieht','Mio','kommt','insgesamt','Ve','sogar','große','deutlich ','sagen','kommen','dabei','vielen','vier','aufgrund','bzw','sollen','lange','worden','davon','konnte','Donald','lassen','bleiben','bleibt','derzeit','geworden','ersten','allerdings','sehen','größten','fast','Tag','sollten','wenig','darauf','Angela','heißt','zehn','gilt','letzten','wegen','stehen','jedoch','Dezember','denen','İş','Bankası','Beispiel','liegt','Seite','Woche','macht','eigenen','tun','stellen','nie','a','sowie','jahr','Jahr','t€','T€','gfr','mehr','prozent','uhr','sei','ab','AG','T','gut','mal','seit','sagt','gibt','ganz','wer','ende','teur','rund','jahr','ja','dach','DACH','beim','jahr','geht','gar','zwei','erst','etwa'))\n",
    "\n",
    "    sents=sent_tokenize(text)\n",
    "    for sent in sents:\n",
    "        w_list = []\n",
    "        #分词、去除停用词\n",
    "        word_tokens=word_tokenize(sent)\n",
    "        for w in word_tokens:\n",
    "            if (w not in stop_words) and w[0].isalpha():\n",
    "                w_list.append(w)\n",
    "        co_ist.append(' '.join(w_list))\n",
    "    #print(co_ist)\n",
    "    \n",
    "def str2csv(filePath, s, x):\n",
    "    '''\n",
    "    将字符串写入到本地csv文件中\n",
    "    :param filePath: csv文件路径\n",
    "    :param s: 待写入字符串(逗号分隔格式)\n",
    "    '''\n",
    "    if x=='node':\n",
    "        with open(filePath, 'w', encoding='utf-8') as f:\n",
    "            f.write(\"Label,Weight\\r\")\n",
    "            f.write(s)\n",
    "        print('写入文件成功,请在'+filePath+'中查看')\n",
    "    else:\n",
    "        with open(filePath, 'w', encoding='utf-8') as f:\n",
    "            f.write(\"Source,Target,Weight\\r\")\n",
    "            f.write(s)\n",
    "        print('写入文件成功,请在'+filePath+'中查看')\n",
    "\n",
    "def sortDictValue(dict, is_reverse):\n",
    "    '''\n",
    "    将字典按照value排序\n",
    "    :param dict: 待排序的字典\n",
    "    :param is_reverse: 是否按照倒序排序\n",
    "    :return s: 符合csv逗号分隔格式的字符串\n",
    "    '''\n",
    "    # 对字典的值进行倒序排序,items()将字典的每个键值对转化为一个元组,key输入的是函数,item[1]表示元组的第二个元素,reverse为真表示倒序\n",
    "    tups = sorted(dict.items(), key=lambda item: item[1], reverse=is_reverse)\n",
    "    s = ''\n",
    "    for tup in tups:  # 合并成csv需要的逗号分隔格式\n",
    "        s = s + tup[0] + ',' + str(tup[1]) + '\\n'\n",
    "    return s\n",
    "\n",
    "def build_matrix(co_authors_list, is_reverse):\n",
    "    '''\n",
    "    根据共同列表,构建共现矩阵(存储到字典中),并将该字典按照权值排序\n",
    "    :param co_authors_list: 共同列表\n",
    "    :param is_reverse: 排序是否倒序\n",
    "    :return node_str: 三元组形式的节点字符串(且符合csv逗号分隔格式)\n",
    "    :return edge_str: 三元组形式的边字符串(且符合csv逗号分隔格式)\n",
    "    '''\n",
    "    node_dict = {}  # 节点字典,包含节点名+节点权值(频数)\n",
    "    edge_dict = {}  # 边字典,包含起点+目标点+边权值(频数)\n",
    "    # 第1层循环,遍历整表的每行信息\n",
    "    for row_authors in co_authors_list:\n",
    "        row_authors_list = row_authors.split(' ') # 依据','分割每行,存储到列表中\n",
    "        # 第2层循环\n",
    "        for index, pre_au in enumerate(row_authors_list): # 使用enumerate()以获取遍历次数index\n",
    "            if pre_au in useful_words:\n",
    "                # 若遍历到倒数第一个元素,则无需记录关系,结束循环即可\n",
    "                if pre_au == row_authors_list[-1]:\n",
    "                    break\n",
    "                connect_list = row_authors_list[index+1:]\n",
    "                # 第3层循环,遍历当前行词后面所有的词,以统计两两词出现的频次\n",
    "                for next_au in connect_list:\n",
    "                    A, B = pre_au, next_au\n",
    "                    #只有当A或B出现频率较高时，才加入边\n",
    "                    if A in useful_words and B in useful_words:\n",
    "                        # 统计单个词出现的频次\n",
    "                        if pre_au not in node_dict:\n",
    "                            node_dict[pre_au] = 1\n",
    "                        else:\n",
    "                            node_dict[pre_au] += 1\n",
    "                        if next_au not in node_dict:\n",
    "                            node_dict[next_au] = 1\n",
    "                        else:\n",
    "                            node_dict[next_au] += 1\n",
    "\n",
    "                        # 固定两两词的顺序\n",
    "                        # 仅计算上半个矩阵\n",
    "                        if A==B:\n",
    "                            continue\n",
    "                        if A > B:\n",
    "                            A, B = B, A\n",
    "                        key = A+','+B  # 格式化为逗号分隔A,B形式,作为字典的键\n",
    "                        # 若该关系不在字典中,则初始化为1,表示词间的共同出现次数\n",
    "                        if key not in edge_dict:\n",
    "                            edge_dict[key] = 1\n",
    "                        else:\n",
    "                            edge_dict[key] += 1\n",
    "    #减少edge_dict中键值对，以降低时间复杂度\n",
    "    new_edge_dict={}\n",
    "    for key in edge_dict:\n",
    "        if edge_dict[key] > 5:\n",
    "            new_edge_dict[key]=edge_dict[key]\n",
    "    print(len(new_edge_dict))\n",
    "\n",
    "    # 对得到的字典按照value进行排序\n",
    "    #with open('./test.txt', 'w', encoding='utf-8') as f:\n",
    "    #    # 将dic dumps json 格式进行写入\n",
    "    #    f.write(json.dumps(new_edge_dict))\n",
    "    node_str = sortDictValue(node_dict, is_reverse)  # 节点\n",
    "    edge_str = sortDictValue(new_edge_dict, is_reverse)   # 边\n",
    "    return node_str, edge_str\n",
    "\n",
    "def main():\n",
    "    # 从语料库读取疫情后的文本\n",
    "    with open(\"myCorpus_after.txt\",encoding=\"utf-8\") as f:\n",
    "        afterText = f.read()\n",
    "\n",
    "    # 获取数据并存储到列表中\n",
    "    my_cut(afterText)\n",
    "\n",
    "    # 根据共同词列表, 构建共现矩阵(存储到字典中), 并将该字典按照权值排序\n",
    "    node_str, edge_str = build_matrix(co_ist, is_reverse=True)\n",
    "    #print(edge_str)\n",
    "\n",
    "    # 将字符串写入到本地csv文件中\n",
    "    filePath1 = 'node_after.csv'\n",
    "    filePath2 = 'edge_after.csv'\n",
    "    str2csv(filePath1,node_str,'node')\n",
    "    str2csv(filePath2,edge_str,'edge')\n",
    "\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "49cb93f377a7abe7414b7b0f21fb3017538004a126cf690fb524202736b7fb92"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
